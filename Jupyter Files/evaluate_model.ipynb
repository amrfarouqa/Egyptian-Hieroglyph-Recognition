{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('../PKL Files/english-hiero-both.pkl')\n",
    "train = load_clean_sentences('../PKL Files/english-hiero-train.pkl')\n",
    "test = load_clean_sentences('../PKL Files/english-hiero-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare hieroglyph tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[N29Q3D36S29D36N37 V28AA1N29Q3I9V28D058 X1D36 Z4G01V28 M17AAA1 G01M170AA1V31N29 I9N29], target=[Theres nothing we can do about it], predicted=[theres nothing we can do about it]\n",
      "src=[I9N29 X1G01N37 N29D36S29S29I9M170D21D46 Z4AA1D21M17A D46D36N37N29D36S29M17AG01D46], target=[It was terribly cold yesterday], predicted=[it was terribly cold yesterday]\n",
      "src=[I9 Q3G01W11D36 V28AA1 N37N29G01N29I9N37N29I9Z4N37], target=[I have no statistics], predicted=[i have no statistics]\n",
      "src=[D46AA1V31S29D36 D058AA1I9V28D058 N29AA1AA1 G43G01N37N29], target=[Youre going too fast], predicted=[youre going too fast]\n",
      "src=[N29AA1O4N37 D36D46D36N37 G01S29D36 M170D21V31D36], target=[Toms eyes are blue], predicted=[toms eyes are blue]\n",
      "src=[Q3D36 I9N37 N29Q3D36 D21G01N37N29 O4G01V28 I9 X1G01V28N29 N29AA1 N37D36D36], target=[He is the last man I want to see], predicted=[he is the last man i want to see]\n",
      "src=[G01D21D21 AA1G43 N29Q3D36N37D36 O4D36D36N29I9V28D058N37 G01S29D36 I9V28 D36V28D058D21I9N37Q3], target=[All of these meetings are in English], predicted=[all of these meetings are in english]\n",
      "src=[D46AA1V31 O4I9D058Q3N29 M170D36 N29Q3D36 AA1V28D21D46 AA1V28D36 Q3D36S29D36 X1Q3AA1 Z4G01V28 M17AAA1 N29Q3G01N29], target=[You might be the only one here who can do that], predicted=[you might be the only one here who can do that]\n",
      "src=[N37Q3D36 G43D36D21N29 N37AA1O4D36N29Q3I9V28D058 M170D36N29X1D36D36V28 D21AA1W11D36 G01V28M17A Q3G01N29S29D36M17A], target=[She felt something between love and hatred], predicted=[she felt something between love and hatred]\n",
      "src=[Z4Q3I9D21M17AS29D36V28 V28D36D36M17A N29AA1 F32D21G01D46], target=[Children need to play], predicted=[children need to play]\n",
      "BLEU-1: 0.799069\n",
      "BLEU-2: 0.766203\n",
      "BLEU-3: 0.751174\n",
      "BLEU-4: 0.681048\n",
      "test\n",
      "src=[G43G01Z4D36M170AA1AA1N35 I9N37 M170AA1S29I9V28D058], target=[Facebook is boring], predicted=[boxing is boring boring]\n",
      "src=[N29Q3G01N29 I9N37 V28D36X1 G01 N37Q3AA1F32 X1Q3I9Z4Q3 AA1F32D36V28D36M17A D21G01N37N29 X1D36D36N35], target=[That is new a shop which opened last week], predicted=[that is a new book she passed last week]\n",
      "src=[X1D36D21D21 Z4Q3D36Z4N35], target=[Well check], predicted=[well check]\n",
      "src=[I9 M17AAA1V28N29 S29D36G01D21D21D46 D21I9N35D36 G01V28D46 AA1G43 N29Q3G01N29], target=[I dont really like any of that], predicted=[i dont really like any of that]\n",
      "src=[D46AA1V31S29 G01V28N37X1D36S29 I9N37 G43G01S29 G43S29AA1O4 N37G01N29I9N37G43G01Z4N29AA1S29D46], target=[Your answer is far from satisfactory], predicted=[your answer is far from satisfactory]\n",
      "src=[Q3D36 D21AA1AA1N35N37 D36V13G01Z4N29D21D46 D21I9N35D36 Q3I9N37 M170S29AA1N29Q3D36S29], target=[He looks exactly like his brother], predicted=[he looks exactly like his brother]\n",
      "src=[D46AA1V31 V28D36D36M17A N29AA1 M170D36 O4AA1S29D36 G01N29N29D36V28N29I9W11D36 I9V28 Z4D21G01N37N37], target=[You need to be more attentive in class], predicted=[you need to be more careful in class]\n",
      "src=[F32D36AA1F32D21D36 I9V28 N29Q3D36N37D36 G01S29D36G01N37 G01S29D36 D058S29AA1X1I9V28D058 Q3V31V28D058S29I9D36S29 D36G01Z4Q3 D46D36G01S29], target=[People in these areas are growing hungrier each year], predicted=[in in these these are growing than every year]\n",
      "src=[D36W11D36S29D46M170AA1M17AD46 I9N37 N37V31F32F32AA1N37D36M17A N29AA1 X1D36G01S29 G01 N29I9D36 G01N29 N29Q3D36 F32G01S29N29D46], target=[Everybody is supposed to wear a tie at the party], predicted=[everybody is supposed to wear a at at the party]\n",
      "src=[X1AA1V31D21M17A D46AA1V31 D21I9N35D36 N29AA1 N35V28AA1X1 Q3AA1X1 I9 M17AI9M17A N29Q3G01N29], target=[Would you like to know how I did that], predicted=[would you like to know how i did that]\n",
      "BLEU-1: 0.689399\n",
      "BLEU-2: 0.612360\n",
      "BLEU-3: 0.571252\n",
      "BLEU-4: 0.461764\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('../Model Files/model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
